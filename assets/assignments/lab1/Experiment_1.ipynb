{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1449651701571a81",
   "metadata": {},
   "source": [
    "# Experiment 2: Linear Models\n",
    "\n",
    "12th March 2024\n",
    "\n",
    "## Description\n",
    "\n",
    "In this experiment, you will implement linear models, including linear regression, ridge regression, and logistic regression. You will also compare your implementation with the models in `sklearn`.\n",
    "\n",
    "Write your code between the lines `########## Start/End of Your Code ##########`.\n",
    "\n",
    "You should add some comments to keep readability of your code. If your code is not understandable, you will not receive full marks even if your results are correct.\n",
    "\n",
    "You can add some markdown cells to explain your code if necessary. \n",
    "\n",
    "You can also add more cells if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119cbc2af26b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# If you have not installed the libraries, you can install them by running `!pip install <name>` in a code cell\n",
    "# You can add more libraries here if necessary, but avoid using highly integrated, function-calling libraries, e.g., sklearn, PyTorch, Tensorflow, etc.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef3fb7a7eabf42",
   "metadata": {},
   "source": [
    "## Task 1: Linear Regression\n",
    "\n",
    "Recall that in linear regression, we map a input vector $\\boldsymbol{x}$ to a scalar output $y$ using the model\n",
    "\n",
    "$$ y = \\boldsymbol{w}^T \\boldsymbol{x} + b $$\n",
    "\n",
    "where $\\boldsymbol{w}$ is the weight vector and $b$ is the bias term. To optimize the model, we may solve the following optimization problem with Gradient Descent (GD):\n",
    "\n",
    "$$ \\min_{\\boldsymbol{w}, b} \\ell(\\boldsymbol{w}, b)=\\frac{1}{2n} \\sum_{i=1}^n (y_i - \\boldsymbol{w}^T \\boldsymbol{x}_i - b)^2 $$\n",
    "\n",
    "where $(\\boldsymbol{x}_i, y_i)$ is the $i$-th training example. In addition, the above optimization problem can also be written in matrix form:\n",
    "\n",
    "$$ \\min_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta})=\\frac{1}{2n} \\|\\mathbf{X}\\boldsymbol{\\beta}-\\boldsymbol{y}\\|^2 $$\n",
    "\n",
    "where $\\mathbf{X}$ is the feature matrix and $\\boldsymbol{y}$ is the target vector. To solve the optimization problem, we can use the GD method to update the parameters iteratively until convergence:\n",
    "\n",
    "$$ \\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta} - \\alpha \\nabla J(\\boldsymbol{\\beta}) $$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\nabla J(\\boldsymbol{\\beta})$ is the gradient of the loss function with respect to $\\boldsymbol{\\beta}$.\n",
    "\n",
    "In this task, we will implement a simple linear regression model to predict the disease progress of diabetes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7150a84b19044e",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The data set used in this task comes from the \"Diabetes Dataset\". Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. The dataset is from [URL](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html). You can refer to [URL](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset) for more information as well.\n",
    "\n",
    "The dataset is ready for you and is stored in the file `diabetes_train.csv` and `diabetes_test.csv`. Each row in the file is a sample, with the last column being the label (disease progression one year after baseline) and the first 10 columns being the features.\n",
    "\n",
    "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of samples (i.e. the sum of squares of each column totals 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Requirements:\n",
    "# - You can use the pandas library to load the data in 'diabetes_train.csv' and 'diabetes_test.csv' into a DataFrame\n",
    "# - Save the training and test set features as X_train, X_test respectively, type: DataFrame\n",
    "# - Save the training and test set targets as y_train, y_test respectively, type: Series\n",
    "# - Concatenate X_train, X_test into a new variable named X for future use, type: DataFrame\n",
    "# - Concatenate y_train, y_test into a new variable named y for future use, type: Series\n",
    "\n",
    "########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "########## End of Your Code ##########\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X_train.shape)  # (354, 10)\n",
    "print(y_train.shape)  # (354,)\n",
    "print(X_test.shape)  # (88, 10)\n",
    "print(y_test.shape)  # (88,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655617d35739781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the training set\n",
    "X_train.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03fd5864ee9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some insights of the training set\n",
    "X_train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6dc74b3719e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some insights of the training set\n",
    "y_train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013a85f9b0bf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into numpy arrays for further operations\n",
    "# numpy arrays are more useful for mathematical operations\n",
    "# Requirements:\n",
    "# - Transform X_train, X_test, y_train, y_test into numpy arrays\n",
    "# - insert a column of ones to the feature matrices to simplify the calculation of the bias term\n",
    "# - Save the transformed training and test set features still as X_train, X_test respectively, type: ndarray\n",
    "# - Save the transformed training and test set targets still as y_train, y_test respectively, type: ndarray\n",
    "\n",
    "########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "########## End of Your Code ##########\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X_train.shape)  # (354, 11)\n",
    "print(y_train.shape)  # (354,)\n",
    "print(X_test.shape)  # (88, 11)\n",
    "print(y_test.shape)  # (88,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5621add513c6570",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "\n",
    "Now we will implement the linear regression model. The model should be able to:\n",
    "- Fit the training set\n",
    "- Make predictions on the test set\n",
    "- Calculate the mean squared error (MSE) of the test set\n",
    "\n",
    "In this task, you are asked to use the Gradient Descent (GD) method to solve the optimization problem. You should not use the closed-form solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4335d5ba7f77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the linear regression model\n",
    "# Requirements:\n",
    "# - Write the three methods for the LinearRegressionModel class: fit, predict, score\n",
    "# - The fit method should use the GD method to solve the optimization problem\n",
    "# - You can add more methods to the class if you would like to\n",
    "\n",
    "# Tips:\n",
    "# - Gradient Descent may take a long time, you can print the MSE in training process to check if the model is learning\n",
    "# - You can tune the learning rate if the model is not learning well\n",
    "# - A better initialization of the weights and bias can lead to faster convergence. You can initialize \n",
    "# the bias term to the mean of the targets for quicker convergence (for this dataset only)\n",
    "# - Do not worry if the model is not learning well, just keep it and do not spend too much time on it\n",
    "\n",
    "class LinearRegressionModel:\n",
    "    def __init__(self, alpha, threshold, max_iter):\n",
    "        # These hyperparameters are used to control the model training process, you can add more or delete some\n",
    "        self.beta = None  # weights and bias, parameters of the model\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.threshold = threshold  # threshold for the stopping condition (you can self-define the stopping condition)\n",
    "        self.max_iter = int(max_iter)  # maximum number of iterations\n",
    "\n",
    "    def fit(self, X, y):  # Fit the training data given training data X and targets y\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):  # Make predictions on unseen test data X\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):  # Predict and compare to true labels y\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113603578762d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the linear regression model defined by yourself to fit the training set\n",
    "alpha, threshold, max_iter = 1e-2, 1e-4, 1e7  # You can change these hyperparameters\n",
    "linear_regression_model = LinearRegressionModel(alpha, threshold, max_iter)\n",
    "linear_regression_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f4e83b35fb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the performance of the model\n",
    "print(linear_regression_model.score(X_train, y_train))\n",
    "print(linear_regression_model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e78d1eb7f60da",
   "metadata": {},
   "source": [
    "## Task 2: Ridge Regression\n",
    "\n",
    "\n",
    "In short, the ridge regression is just the linear regression with an extra regular term added. To better optimize the model, we may solve the following optimization problem with Gradient Descent (GD):\n",
    "\n",
    "$$ \\min_{\\boldsymbol{w}, b} \\ell(\\boldsymbol{w}, b)=\\frac{1}{2n} \\sum_{i=1}^n (y_i - \\boldsymbol{w}^T \\boldsymbol{x}_i - b)^2  + \\lambda \\sum_{j=1}^p \\boldsymbol{\\beta}_{j}^2$$\n",
    "\n",
    "where $\\lambda$ is a hyperparameter serving as the coefficient of the regularization term, and $\\boldsymbol{\\beta} = \\{ \\boldsymbol{\\beta}_{0}, \\boldsymbol{\\beta}_{1}, ..., \\boldsymbol{\\beta}_{p}  \\}$ is the model parameter vector. In addition, the above optimization problem can also be written in matrix form:\n",
    "\n",
    "$$ \\min_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta})=\\frac{1}{2n} \\|\\mathbf{X}\\boldsymbol{\\beta}-\\boldsymbol{y}\\|^2  + \\lambda \\sum_{j=1}^p \\boldsymbol{\\beta}_{j}^2$$\n",
    "\n",
    "where all other symbols have the same meaning as in Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f0c22",
   "metadata": {},
   "source": [
    "### Data Loading \n",
    "\n",
    "For convenience, you can directly use the data loaded in Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825e6de3",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "\n",
    "Now we will implement the ridge regression model. The model should be able to:\n",
    "- Fit the training set\n",
    "- Make predictions on the test set\n",
    "- Calculate the mean squared error (MSE) of the test set\n",
    "\n",
    "In this task, you are asked to use the Gradient Descent (GD) method to solve the optimization problem. You should not use the closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08215e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the ridge regression model\n",
    "# Requirements:\n",
    "# - Write the three methods for the RidgeRegressionModel class: fit, predict, score\n",
    "# - The fit method should use the GD method to solve the optimization problem\n",
    "# - You can add more methods to the class if you would like to\n",
    "\n",
    "# Tips:\n",
    "# - Gradient Descent may take a long time, you can print the MSE in training process to check if the model is learning\n",
    "# - You can tune the learning rate if the model is not learning well\n",
    "# - A better initialization of the weights and bias can lead to faster convergence. You can initialize \n",
    "# the bias term to the mean of the targets for quicker convergence (for this dataset only)\n",
    "# - Do not worry if the model is not learning well, just keep it and do not spend too much time on it\n",
    "\n",
    "class RidgeRegressionModel:\n",
    "    def __init__(self, alpha, threshold, max_iter, lambda_value):\n",
    "        # These hyperparameters are used to control the model training process, you can add more or delete some\n",
    "        self.beta = None  # weights and bias, parameters of the model\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.threshold = threshold  # threshold for the stopping condition (you can self-define the stopping condition)\n",
    "        self.max_iter = int(max_iter)  # maximum number of iterations\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def fit(self, X, y):  # Fit the training data given training data X and targets y\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):  # Make predictions on unseen test data X\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):  # Predict and compare to true labels y\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the ridge regression model defined by yourself to fit the training set\n",
    "alpha, threshold, max_iter, lambda_value = 1e-2, 1e-4, 1e7, 1e-4 # You can change these hyperparameters\n",
    "ridge_regression_model = RidgeRegressionModel(alpha, threshold, max_iter, lambda_value)\n",
    "ridge_regression_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the performance of the model\n",
    "print(ridge_regression_model.score(X_train, y_train))\n",
    "print(ridge_regression_model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7addbaa",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "In Task 1, we used a fixed number of top 354 to divide the training set and the test set, but this division may be biased and will also bias the evaluation of the model and hyperparameters. Therefore, in order to minimize the bias caused by data partition as much as possible, we use $k$-fold cross-validation and take $k=5$ here.\n",
    "- Divide the data into $k$ folds\n",
    "- Use cross validation in pipeline\n",
    "- Visualize the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the shuffled dataset into k folds\n",
    "k = 5\n",
    "n_samples = X.shape[0]\n",
    "fold_size = math.ceil(n_samples/ k)\n",
    "\n",
    "# Add a column of ones to the feature matrices to simplify the calculation of the bias term\n",
    "X['bias'] = 1\n",
    "\n",
    "# Shuffle both X and Y using the same permutation of indices\n",
    "indices = np.random.permutation(n_samples)\n",
    "X = X.iloc[indices]\n",
    "y = y.iloc[indices]\n",
    "\n",
    "X_folds = []\n",
    "y_folds = []\n",
    "start = 0\n",
    "for i in range(k):\n",
    "    if i == k - 1:\n",
    "        X_folds.append(X[start:])\n",
    "        y_folds.append(y[start:])\n",
    "    else :\n",
    "        end = start + fold_size\n",
    "        X_folds.append(X[start:end])\n",
    "        y_folds.append(y[start:end])\n",
    "        start = end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate lambda for the ridge regression model\n",
    "# Requirements:\n",
    "# - We have provided a pipeline function for you below\n",
    "# - Fill in the blank inside the function\n",
    "\n",
    "def pipline_crossval(alpha, threshold, max_iter, lambda_value_list, X_folds, y_folds): \n",
    "    train_loss_list = []\n",
    "    test_loss_list  = []\n",
    "    \n",
    "    for lambda_value in lambda_value_list:\n",
    "        train_loss = []  # store the loss of each fold\n",
    "        test_loss  = []\n",
    "        \n",
    "        for i in range(len(X_folds)):\n",
    "            X_Train = X_folds[:i] + X_folds[i + 1:]\n",
    "            X_Train = np.concatenate(X_Train, axis=0)\n",
    "            y_Train = y_folds[:i] + y_folds[i + 1:]\n",
    "            y_Train = np.concatenate(y_Train, axis=0)\n",
    "            ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "            ########## End of Your Code ##########\n",
    "        \n",
    "        train_loss_list.append(np.mean(train_loss))  # store the average loss of all folds as the final loss for the current lambda\n",
    "        test_loss_list.append(np.mean(test_loss))  \n",
    "    \n",
    "    # Draw the loss curve\n",
    "    plt.plot([-math.log10(x) for x in lambda_value_list], train_loss_list, linestyle=\"-\", marker=\"o\", label=\"train_loss\", linewidth=1)\n",
    "    plt.plot([-math.log10(x) for x in lambda_value_list], test_loss_list, linestyle=\"-\", marker=\"o\", label=\"test_loss\", linewidth=1)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Log(Lambda Value)',font={'family':'Times New Roman', 'size':25})\n",
    "    plt.xticks(fontsize=25)\n",
    "    plt.locator_params(axis='x', nbins=5)\n",
    "    plt.ylabel('Loss',font={'family':'Times New Roman', 'size':25})\n",
    "    plt.yticks(fontsize=25)\n",
    "    plt.locator_params(axis='y', nbins=5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905013c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate lambda for the ridge regression model\n",
    "lambda_value_list = [10 ** (-i) for i in range(7)]  # Possible lambda values, you can change this list\n",
    "alpha, threshold, max_iter= 1e-2, 1e-4, 1e7  # You can change these hyperparameters\n",
    "pipline_crossval(alpha, threshold, max_iter, lambda_value_list, X_folds, y_folds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241e7f48667de61",
   "metadata": {},
   "source": [
    "## Task 3: Logistic Regression\n",
    "\n",
    "Recall that in logistic regression, we map a input vector $\\boldsymbol{x}$ to a posterior probability $p(y|\\boldsymbol{x})$ using the model\n",
    "\n",
    "$$ p(y=1|\\boldsymbol{x}) = \\frac{1}{1+e^{-(\\boldsymbol{w}^\\text{T} \\boldsymbol{x} + b)}} $$\n",
    "\n",
    "where $\\boldsymbol{w}$ is the weight vector and $b$ is the bias term, and return whichever label ($y$ = 1 or $y$ = 0) is higher probability. To optimize the model, we may solve the following optimization problem with Gradient Descent (GD):\n",
    "\n",
    "$$ \\min_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta})=\\frac{1}{n} \\sum_{i=1}^n(-y_i \\boldsymbol{\\beta}^\\text{T} \\boldsymbol{\\hat{x}}_i + \\ln(1+e^{\\boldsymbol{\\beta}^\\text{T} \\boldsymbol{\\hat{x}}_i}))$$\n",
    "\n",
    "where $(\\boldsymbol{x}_i, y_i)$ is the $i$-th training example, $\\boldsymbol{\\beta}=(\\boldsymbol{w};b)$ and $\\boldsymbol{\\hat{x}}=(\\boldsymbol{x};1)$. To solve the optimization problem, we can use the GD method to update the parameters iteratively until convergence:\n",
    "\n",
    "$$ \\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta} - \\alpha \\nabla J(\\boldsymbol{\\beta}) $$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\nabla J(\\boldsymbol{\\beta})$ is the gradient of the loss function with respect to $\\boldsymbol{\\beta}$.\n",
    "\n",
    "In this task, we will implement a logistic regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d9710e1b354ab",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "The dataset is ready for you and is stored in the file `pendigit_train.csv` and `pendigit_test.csv`. Each row in the file is a sample, with the last column being the label  and the first 16 columns being the features.\n",
    "\n",
    "Note: Each of these 16 feature variables have been normalized to 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Requirements:\n",
    "# - You can use the pandas and numpy to load the data in 'pendigit_train.csv' and 'pendigit_test.csv'into the ndarray type.\n",
    "# - insert a column of ones to the feature matrices to simplify the calculation of the bias term\n",
    "# - Save the training and test set features as X_train, X_test respectively, type: ndarray\n",
    "# - Save the training and test set targets as y_train, y_test respectively, type: ndarray\n",
    "\n",
    "########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "########## End of Your Code ##########\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X_train.shape)  # (1828, 17)\n",
    "print(y_train.shape)  # (1828,)\n",
    "print(X_test.shape)  # (457, 17)\n",
    "print(y_test.shape)  # (457,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265ded0c62ef82",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "\n",
    "Now we will implement the logistic regression model. The model should be able to:\n",
    "- Fit the training set\n",
    "- Make predictions on the test set\n",
    "- Calculate the accuracy (ACC) of the test set\n",
    "\n",
    "In this task, you are asked to use the Gradient Descent (GD) method to solve the optimization problem. You should not use the closed-form solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eda881e3815201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the logistic regression model\n",
    "# Requirements:\n",
    "# - Write the three methods for the LogisticRegressionModel class: fit, predict, predict_prob, score, sigmoid\n",
    "# - The fit method should use the GD method to solve the optimization problem\n",
    "# - You can add more methods to the class if you would like to\n",
    "\n",
    "# Tips:\n",
    "# - Gradient Descent may take a long time, you can print the related information in training process to check if the model is learning\n",
    "# - You can tune the learning rate if the model is not learning well\n",
    "# - A better initialization of the weights and bias can lead to faster convergence. You can initialize \n",
    "# the bias term to the mean of the targets for quicker convergence (for this dataset only)\n",
    "# - Do not worry if the model is not learning well, just keep it and do not spend too much time on it\n",
    "\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, alpha, threshold, max_iter):\n",
    "        self.beta = None  # weights and bias, parameters of the model\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.threshold = threshold  # threshold for the stopping condition (you can self-define the stopping condition)\n",
    "        self.max_iter = int(max_iter)  # maximum number of iterations\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):  # Fit the training data given training data X and targets y\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):  # Make predictions on unseen test data X\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_prob(self, X): # Make predicted probabilities on unseen test data X\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return y_prob\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):  # Predict and compare to true labels y\n",
    "        ########## Start of Your Code ##########\n",
    "\n",
    "\n",
    "        ########## End of Your Code ##########\n",
    "        return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29406af25a1db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the logistic regression model defined by yourself to fit the training set\n",
    "alpha, threshold, max_iter = 1e-2, 1e-4, 1e7  # You can change these hyperparameters\n",
    "logistic_regression_model = LogisticRegressionModel(alpha, threshold, max_iter)\n",
    "logistic_regression_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc59f5b9a137f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logistic_regression_model.score(X_train, y_train))\n",
    "print(logistic_regression_model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb12d2d8ae2b8d",
   "metadata": {},
   "source": [
    "## Task 4: Comparing with the models in sklearn\n",
    "Now that you have completed the linear regression, ridge regression and logistic regression implementation, compare these with standard models in sklearn. Then, discuss the difference between them and try to analyze the possible reasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909b9912f682fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison\n",
    "# Requirements:\n",
    "# - Compare the models you implemented with the models in sklearn with yours, and try to analyze the possible reasons for the difference\n",
    "# - You can use sklearn in this task\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "########## Start of Your Code ##########\n",
    "\n",
    "########## End of Your Code ##########\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
